name: Test Verification with Hallucination Detection

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.10'

jobs:
  test-and-verify:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install -e ".[dev]"
    
    - name: Run tests with JSON report
      run: |
        pytest --json-report --json-report-file=test-results.json || true
    
    - name: Create verified test results
      run: |
        claude-test-reporter verify-test-results test-results.json \
          --output verified-results.json \
          --format json
    
    - name: Generate HTML report
      run: |
        claude-test-reporter from-pytest test-results.json \
          --output test-report.html \
          --project ${{ github.repository }}
    
    - name: Analyze with LLM (if API key is set)
      if: env.GEMINI_API_KEY != ''
      env:
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      run: |
        claude-test-reporter llm-analyze test-results.json \
          ${{ github.repository }} \
          --output llm-analysis.json \
          --model gemini-2.5-pro
    
    - name: Create LLM prompt for verification
      run: |
        claude-test-reporter create-llm-prompt test-results.json \
          --output test-prompt.txt
    
    - name: Upload test artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-reports
        path: |
          test-results.json
          verified-results.json
          test-report.html
          llm-analysis.json
          test-prompt.txt
    
    - name: Check test results
      run: |
        python -c "
        import json
        with open('verified-results.json') as f:
            results = json.load(f)
        failed = results['immutable_facts']['failed_count']
        if failed > 0:
            print(f'âŒ {failed} tests failing - deployment blocked')
            print('Failed tests:')
            for test in results['failed_test_details']:
                print(f'  - {test[\"name\"]}')
            exit(1)
        else:
            print('âœ… All tests passing - deployment allowed')
        "
    
    - name: Comment PR with results (if PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const results = JSON.parse(fs.readFileSync('verified-results.json', 'utf8'));
          const facts = results.immutable_facts;
          
          const comment = `## ðŸ§ª Test Results (Verified)
          
          **Verification Hash:** \`${results.verification.hash.substring(0, 16)}...\`
          
          | Metric | Value |
          |--------|-------|
          | Total Tests | ${facts.total_test_count} |
          | Passed | ${facts.passed_count} |
          | Failed | ${facts.failed_count} |
          | Success Rate | ${facts.exact_success_rate}% |
          | Deployment | ${facts.deployment_allowed ? 'âœ… ALLOWED' : 'âŒ BLOCKED'} |
          
          ${facts.failed_count > 0 ? '### Failed Tests\n' + results.failed_test_details.map(t => `- ${t.name}`).join('\n') : ''}
          
          ---
          *This is a cryptographically verified test report. Any claims contradicting these facts should be considered hallucinations.*`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  hallucination-monitor:
    runs-on: ubuntu-latest
    needs: test-and-verify
    if: always()
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Download test artifacts
      uses: actions/download-artifact@v3
      with:
        name: test-reports
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install claude-test-reporter
      run: |
        python -m pip install --upgrade pip
        pip install -e .
    
    - name: Monitor for hallucinations in logs
      run: |
        # This would check any agent or LLM outputs for hallucinations
        # For demo purposes, we'll create a sample check
        
        echo "Checking for common hallucination patterns..."
        
        # Create a sample response to check
        echo "The tests are mostly passing with approximately 95% success rate." > agent_response.txt
        
        # Check for hallucinations
        claude-test-reporter check-hallucination \
          verified-results.json \
          agent_response.txt \
          --output hallucination-report.json || echo "Hallucinations detected!"
    
    - name: Upload hallucination report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: hallucination-monitoring
        path: hallucination-report.json